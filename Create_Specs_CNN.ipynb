{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"colab":{"name":"Create_Specs_CNN.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"collapsed":true,"id":"AUAJWH9y6OYm"},"source":["#Import required packages\n","import pandas as pd\n","import numpy as np\n","import librosa\n","import pathlib\n","import os\n","from google.colab import drive\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import scale\n","import warnings\n","import glob\n","from scipy import signal"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"4AcX48YH6OYp"},"source":["#Read audio classification file and clean\n","audio_class_df = pd.read_csv(\"audioclassification_meta.csv\")\n","c_names = audio_class_df.columns.tolist()\n","c_names = c_names[0].replace(\" \", \"_\").split(\"\\t\")\n","\n","audio_class_df[c_names] = audio_class_df['VoxCeleb1 ID\\tVGGFace1 ID\\tGender\\tNationality\\tSet'].\\\n","                        str.split(\"\\t\", expand = True)\n","audio_class_df = audio_class_df[c_names]\n","\n","#Set as dictionary\n","audio_class_dict = audio_class_df.set_index(\"VoxCeleb1_ID\").T.to_dict('list')\n","\n","#View data\n","audio_class_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"9VgLa-P86OYq"},"source":["# Phoneme data needed to extract phonemes only\n","phoible_df = pd.read_csv(\"phoible.csv\")\n","\n","# Generate a mapping from nationality to language spoken\n","nationalities_to_language = {'Irish': 'English',\n","                             'India': 'Hindi', \n","                             'USA': 'English (American)',\n","                             'Australia': 'English (Australian)',\n","                             'Canada': 'English', \n","                             'UK': 'English (British)', \n","                             'Norway': 'Norwegian',\n","                             'Italy': 'Italian',\n","                             'Sudan': 'Arabic',\n","                             'Mexico': 'Spanish',\n","                             'China': 'Standard Chinese; Mandarin',\n","                             'Switzerland': 'Swiss German',\n","                             'Guyana': 'English',\n","                             'Philippines':'Filipino',\n","                             'New Zealand': 'English (New Zealand)',\n","                             'Germany': 'German', \n","                             'Portugal': 'Portuguese (European)',\n","                             'Netherlands': 'Dutch',\n","                             'Pakistan': 'Urdu',\n","                             'Croatia': 'Croatian',\n","                             'South Korea': 'Korean',\n","                             'Sweden': 'Swedish',\n","                             'Russia': 'Russian',\n","                             'Poland': 'Polish',\n","                             'Sri Lanka': 'Sinhalese', \n","                             'Singapore': 'Mandarin Chinese',\n","                             'Chile': 'Spanish',\n","                             'Spain': 'Spanish',\n","                             'Israel':'Modern Hebrew',\n","                             'Brazil': 'Portuguese (Brazilian)',\n","                             'Trinidad and Tobago': 'English', \n","                             'Denmark': 'Danish',\n","                             'Austria': 'German', \n","                             'South Africa': 'English', \n","                             'Iran': 'Farsi'} \n","\n","# Filter dataframe to only nationalities that will be encountered\n","phoible_df = phoible_df[phoible_df['LanguageName'].isin(list(nationalities_to_language.values()))]\n","\n","# Find all languages spoken within VoxCeleb\n","all_languages = list(phoible_df['LanguageName'].unique())\n","\n","# Define a mapping from language to phoneme \n","# Key is language and value is a set of phonemes within that language\n","phonemes_per_lang = {}\n","for j in range(len(all_languages)):\n","    phonemes_per_lang[all_languages[j]] = {}\n","    phonemes_in_lang = phoible_df[phoible_df['LanguageName'] == all_languages[j]]['Phoneme'].unique()\n","    phonemes_per_lang[all_languages[j]] = set()\n","    for i in range(len(phonemes_in_lang)):\n","        phonemes_per_lang[all_languages[j]].add(phonemes_in_lang[i])\n","\n","# Create a set of all the phonemes in English languages\n","eng_langs = ['English', 'English (American)','English (Australian)', \\\n","             'English (British)', 'English (New Zealand)']\n","english_phonemes = set()\n","for lang in eng_langs:\n","    english_phonemes.update(phonemes_per_lang[lang])\n","\n","# Define a mapping from English phonemes to allophones that may be present in tother languages\n","english_phonemes_to_allophones = {}\n","for phoneme in english_phonemes:\n","    english_phonemes_to_allophones[phoneme] = set(phoneme)                               \n","    for allophones in phoible_df[(phoible_df['Phoneme'] == phoneme) & (phoible_df['LanguageName'].isin(eng_langs))].Allophones:\n","        if pd.isnull(allophones) == False and allophones.isalnum():\n","            for allophone in allophones:\n","                english_phonemes_to_allophones[phoneme].add(allophone)\n","\n","def get_key_english_phonemes_to_allophones(val):\n","    '''\n","    Function: Find English allophones of non-English phonemes\n","    Inputs: \n","        - val: a phoneme\n","    Outputs:\n","        - key: the allophone that phoneme is known as in English, if applicable\n","    '''\n","    for key, value in english_phonemes_to_allophones.items():\n","        if val in value:\n","            return key\n","\n","# Define phonemes that will be unseen in training as they are non-English and \n","# remove them from a language's phoneme set, replaced by an unseen tag\n","for language in phonemes_per_lang:\n","    unseen_phonemes = set()\n","    for phoneme in phonemes_per_lang[language]:\n","        if phoneme not in english_phonemes:\n","            unseen_phonemes.add(phoneme) \n","    for unseen_phoneme in unseen_phonemes:\n","        phonemes_per_lang[language].remove(unseen_phoneme)\n","        possible_allophone = get_key_english_phonemes_to_allophones(unseen_phoneme)\n","        if possible_allophone is not None:\n","            phonemes_per_lang[language].add(possible_allophone)\n","        else:\n","            phonemes_per_lang[language].add('unseen')\n","\n","# Bypass languages to map directly from nationality to phoneme\n","nationalities_to_phonemes = {}\n","for nationality in nationalities_to_language.keys():\n","    nationalities_to_phonemes[nationality] = \\\n","    phonemes_per_lang[nationalities_to_language[nationality]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"w8FiDnvb6OYr"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)\n","%cd \"/content/drive/Shareddrives/CIS_519_Final_Project\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"CVUCDxRD6OYr"},"source":["# Path to audio npz files\n","speech_path = \"/content/drive/Shareddrives/CIS_519_Final_Project/\"\n","# Path to phoneme npz files\n","phoneme_path = \"/content/drive/Shareddrives/CIS_519_Final_Project/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"mFZBRQH96OYs"},"source":["def pull_id_npz(file_name):\n","    '''\n","    Function: Load in files in npz format that are stored as dictionaries\n","    Inputs: \n","        - file_name: string of file name containing path to file\n","    Outputs:\n","        - container_list: nested list containing values in the npz files\n","    '''\n","    container_list = []\n","    container = np.load(file_name,allow_pickle=True)\n","    container_list.append([container[key] for key in container])\n","    return container_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"3IlPsdF86OYs"},"source":["# Unpack npz files storing raw speech\n","def pull_speech_npz(speech_path, id):\n","    '''\n","    Function: Unpack npz files storing raw audio\n","    Inputs: \n","        - speech_path: path to audio npz files\n","    Outputs:\n","        - nested list containing values in the npz files \n","    '''\n","    for path, subdirs, files in os.walk(speech_path):\n","        if \"phoneme\" not in path:\n","            os.chdir(path)\n","        if (id+'.npz') in os.listdir():\n","            return pull_id_npz(id+'.npz')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"3lXcWJMQ6OYs"},"source":["# Pull all the phoneme npz files associated with a given Voxceleb id\n","def pull_phoneme_npz(phoneme_path, id):\n","    '''\n","    Function: Unpack npz files storing phonemes\n","    Inputs: \n","        - speech_path: path to phoneme npz files\n","    Outputs:\n","        - nested list containing values in the npz files \n","    '''\n","    for path, subdirs, files in os.walk(phoneme_path):\n","        if \"phoneme\" in path:\n","            os.chdir(path)\n","        if (id+'.npz') in os.listdir():\n","            return pull_id_npz(id+'.npz')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"HA6sNJla6OYt"},"source":["from python_speech_features import fbank\n","# Function to calculate mfcc coefficients from raw speech, currently using 10 coeffs to limit size\n","def compute_mfcc(data_to_ids_dict,numcep=10,fs=16e3, max_len=10000):\n","    '''\n","    Function: Calculate Mel frequency cepstral coefficients from raw audio npz\n","    Inputs: \n","        - data_to_ids_dict: dictionary of ids to raw audio extracted from npz file\n","    Outputs:\n","        - X_mfcc: list of 13 extract MFCCs per time window limited to length 10,000 per sample\n","    '''\n","    X_mfcc = []\n","    for key in data_to_ids_dict:\n","        if data_to_ids_dict[key] is not None:\n","            for data in data_to_ids_dict[key][0]:\n","                mfccs = np.array(mfcc(data).flatten())\n","                if (max_len > mfccs.shape[0]):\n","                    pad_width = max_len - mfccs.shape[0]\n","                    mfccs = np.pad(mfccs, pad_width=(0, pad_width), mode='constant')\n","                else:\n","                    mfccs = mfccs[:max_len]\n","                X_mfcc.append(mfccs)\n","    return X_mfcc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"jklwTEj-6OYt"},"source":["def extract_phonemes_for_training(phoneme_dict):\n","    '''\n","    Function: Extract phonemes per training instance \n","    Inputs: \n","        - phoneme_dict: dictionary of ids to phoneme sets extracted from npz file\n","    Outputs:\n","        - X: list of sets of extracted phonemes, 1 set per file \n","    '''\n","    X = []\n","    for key in phoneme_dict.keys():\n","        for item in phoneme_dict[key][0]:\n","            X.append(item.item())\n","    return X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"L_c-5pB_6OYu"},"source":["def extract_nationalities(npz_to_speech_ids_dict, audio_class_dict):\n","    '''\n","    Function: Extract nationalities per training instance \n","    Inputs: \n","        - npz_to_speech_ids_dict: dictionary of ids to raw audio extracted from npz file\n","        - audio_class_dict: dictionary of id to nationality \n","    Outputs:\n","        - X: list of sets of extracted nationalities, 1 nationality per file \n","    '''\n","    y = []\n","    for key in npz_to_speech_ids_dict.keys():\n","        y_val = audio_class_dict[key][2]\n","        if npz_to_speech_ids_dict[key] is not None:\n","            for i in range(len(npz_to_speech_ids_dict[key][0])):\n","            y.append(y_val)\n","    return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"vi3M5ESm6OYu"},"source":["# Define all nationalities as English or non English speaking\n","all_nationalities = list(audio_class_df['Nationality'].unique())\n","# Remove English speaking nationalities with only 1 speaker in the set \n","all_nationalities.remove('South Africa')\n","all_nationalities.remove('Guyana')\n","all_nationalities.remove('Trinidad and Tobago')\n","all_nationalities.remove('Germany') # difficulty generating npz files here\n","\n","\n","eng_nationalities =  ['USA', 'UK', 'Australia', 'Canada', 'New Zealand', 'Ireland']\n","non_eng_nationalities = [nationality for nationality in all_nationalities if nationality not in eng_nationalities]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"p3uNvBmp6OYu"},"source":["from python_speech_features import mfcc\n","\n","def generate_zero_shot_data(nationalities):\n","    '''\n","    Function: Extract MFCCs and nationalities for ids belonging to a nationality within \"nationalities\"\n","    Inputs: \n","        - nationalities: a list of nationalities for which to determine MFCCs and nationalities\n","    Outputs:\n","        - X_mfcc_train_flat: list of 10,000 MFCC coefficients per example \n","        - y_nationality_train_flat: list of the nationality corresponding to a given example \n","    '''\n","    X_mfcc_train = []\n","    y_nationality_train = []\n","    for nationality in nationalities:\n","        print('Working on importing ' + nationality)\n","        speech_to_phoneme_training_ids = list(audio_class_df[audio_class_df['Nationality'].isin\\\n","                               ([nationality])].VoxCeleb1_ID)\n","        # Remove corrupt files and limit input sizes\n","        if 'id11240' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id11240')\n","        if 'id10155' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10155') \n","        if 'id10347' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10347')\n","        if 'id10409' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10409')\n","        if 'id10061' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10061') \n","        if len(speech_to_phoneme_training_ids) > 50:\n","            speech_to_phoneme_training_ids = speech_to_phoneme_training_ids[0:49]\n","            \n","        # Define dictionary from id to raw audio        \n","        npz_to_speech_ids_dict = {id: pull_speech_npz(speech_path,id) for id in speech_to_phoneme_training_ids}\n","        # Compute MFCCs off raw audio\n","        mfcc = np.array(compute_mfcc(npz_to_speech_ids_dict))\n","        X_mfcc_train.append(mfcc)\n","        # Extract nationalities from ids \n","        y_nationality = extract_nationalities(npz_to_speech_ids_dict, audio_class_dict)\n","        y_nationality_train.append(y_nationality)\n","        # Delete large files to clear memory\n","        del npz_to_speech_ids_dict\n","        del mfcc\n","        del y_nationality\n","    # Flatten lists\n","    X_mfcc_train_flat = [item for sublist in X_mfcc_train for item in sublist]\n","    y_nationality_train_flat = [item for sublist in y_nationality_train for item in sublist]\n","    return X_mfcc_train_flat, y_nationality_train_flat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"yCDyDVrC6OYv"},"source":["def generate_phoneme_data(nationalities):\n","        '''\n","    Function: Extract MFCCs and nationalities for ids belonging to a nationality within \"nationalities\"\n","    Inputs: \n","        - nationalities: a list of nationalities for which to determine MFCCs and nationalities\n","    Outputs:\n","        - X_mfcc_train_flat: list of 10,000 MFCC coefficients per example \n","        - y_nationality_train_flat: list of the nationality corresponding to a given example \n","    '''\n","    y_phoneme_train = []\n","    for nationality in nationalities:\n","        print('Working on importing ' + nationality)\n","        speech_to_phoneme_training_ids = list(audio_class_df[audio_class_df['Nationality'].isin\\\n","                               ([nationality])].VoxCeleb1_ID)\n","        # Remove corrupt files and limit input sizes\n","        if 'id11240' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id11240')\n","        if 'id10155' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10155') \n","        if 'id10347' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10347')\n","        if 'id10409' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10409')\n","        if 'id10061' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10061') \n","        if len(speech_to_phoneme_training_ids) > 50:\n","            speech_to_phoneme_training_ids = speech_to_phoneme_training_ids[0:49]\n","        # Define dictionary from id to raw audio \n","        npz_to_speech_ids_dict = {id: pull_speech_npz(speech_path,id) for id in speech_to_phoneme_training_ids}\n","        # Extract nationalities from ids \n","        phoneme_ids_dict ={id: pull_phoneme_npz(phoneme_path,id) for id in speech_to_phoneme_training_ids}\n","        y_phonemes = extract_phonemes_for_training(phoneme_ids_dict)    \n","        y_phoneme_train.append(y_phonemes)\n","        # Delete large files to clear memory\n","        del speech_to_phoneme_training_ids\n","        del y_phonemes\n","    # Flatten list\n","    y_phoneme_train_flat = [item for sublist in y_phoneme_train for item in sublist]\n","    return y_phoneme_train_flat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"LJpG4dYP6OYv"},"source":["def generate_specs(nationalities):\n","    X_spec_train = []\n","      for nationality in nationalities:\n","        print('Working on importing ' + nationality)\n","        speech_to_phoneme_training_ids = list(audio_class_df[audio_class_df['Nationality'].isin\\\n","                               ([nationality])].VoxCeleb1_ID)\n","        # Remove corrupt files and limit input sizes\n","        if 'id11240' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id11240')\n","        if 'id10155' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10155') \n","        if 'id10347' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10347')\n","        if 'id10409' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10409')\n","        if 'id10061' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10061') \n","        if len(speech_to_phoneme_training_ids) > 50:\n","            speech_to_phoneme_training_ids = speech_to_phoneme_training_ids[0:49]\n","        # Define dictionary from id to raw audio \n","        npz_to_speech_ids_dict = {id: pull_speech_npz(speech_path,id) for id in speech_to_phoneme_training_ids}\n","        #  Compute specrograms off raw audio\n","        spec = get_feats_mod(npz_to_speech_ids_dict)\n","        X_spec_train.append(spec)\n","        # Delete large files to clear memory\n","        del npz_to_speech_ids_dict\n","        del spec\n","    # Flatten list\n","    X_spec_train_flat = [item for sublist in X_spec_train for item in sublist]\n","    return X_spec_train_flat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"gMv-SICB6OYv"},"source":["# Generate training and testing data\n","X_train, y_train_nationalities = generate_zero_shot_data(eng_nationalities)\n","X_test, y_test_nationalities = generate_zero_shot_data(non_eng_nationalities)\n","y_train_phonemes = generate_phoneme_data(eng_nationalities)\n","X_train_spec = generate_specs(eng_nationalities)\n","X_test_spec = generate_specs(non_eng_nationalities)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"MFZ0IUfu6OYw"},"source":["# Save files for later use \n","%cd \"/content/drive/Shareddrives/CIS_519_Final_Project/train_test_data_to_load_in\"\n","np.savez('X_train_mfcc_no_corrupt.npz', *X_train)\n","np.savez('y_train_nationalities_no_corrupt.npz', *y_train_nationalities)\n","np.savez('X_test_mfcc.npz', *X_test)\n","np.savez('y_test_nationalities.npz', *y_test_nationalities)\n","np.savez('y_train_phonemes_no_corrupt.npz', *y_train_phonemes)\n","np.savez('X_train_spec.npz', *X_train_spec) \n","np.savez('X_test_spec.npz', *X_test_spec) \n"],"execution_count":null,"outputs":[]}]}