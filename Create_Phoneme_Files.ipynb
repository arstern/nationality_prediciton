{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"colab":{"name":"Create_Phoneme_Files.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"collapsed":true,"id":"YmGqJMJrPrwr"},"source":["#Import required packages\n","import pandas as pd\n","import numpy as np\n","import pathlib\n","import os\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import scale\n","import warnings\n","import glob\n","from scipy import signal\n","import re\n","import json\n","import logging\n","import collections\n","import sqlite3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZNXv16YRPrwu","outputId":"059a7a64-ea8b-4e1c-b241-668b5555c037"},"source":["#Read audio classification file and clean\n","audio_class_df = pd.read_csv(\"audioclassification_meta.csv\")\n","c_names = audio_class_df.columns.tolist()\n","c_names = c_names[0].replace(\" \", \"_\").split(\"\\t\")\n","\n","audio_class_df[c_names] = audio_class_df['VoxCeleb1 ID\\tVGGFace1 ID\\tGender\\tNationality\\tSet'].\\\n","                        str.split(\"\\t\", expand = True)\n","audio_class_df = audio_class_df[c_names]\n","\n","#Set as dictionary\n","audio_class_dict = audio_class_df.set_index(\"VoxCeleb1_ID\").T.to_dict('list')\n","\n","#View data\n","audio_class_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>VoxCeleb1_ID</th>\n","      <th>VGGFace1_ID</th>\n","      <th>Gender</th>\n","      <th>Nationality</th>\n","      <th>Set</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>id10001</td>\n","      <td>A.J._Buckley</td>\n","      <td>m</td>\n","      <td>Ireland</td>\n","      <td>dev</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>id10002</td>\n","      <td>A.R._Rahman</td>\n","      <td>m</td>\n","      <td>India</td>\n","      <td>dev</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>id10003</td>\n","      <td>Aamir_Khan</td>\n","      <td>m</td>\n","      <td>India</td>\n","      <td>dev</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>id10004</td>\n","      <td>Aaron_Tveit</td>\n","      <td>m</td>\n","      <td>USA</td>\n","      <td>dev</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>id10005</td>\n","      <td>Aaron_Yoo</td>\n","      <td>m</td>\n","      <td>USA</td>\n","      <td>dev</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  VoxCeleb1_ID   VGGFace1_ID Gender Nationality  Set\n","0      id10001  A.J._Buckley      m     Ireland  dev\n","1      id10002   A.R._Rahman      m       India  dev\n","2      id10003    Aamir_Khan      m       India  dev\n","3      id10004   Aaron_Tveit      m         USA  dev\n","4      id10005     Aaron_Yoo      m         USA  dev"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"VesSYwXLPrww","outputId":"d1b50f37-47b3-4d57-e2ad-b26306e652e4"},"source":["phoible_df = pd.read_csv(\"phoible.csv\")\n","phoible_df.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/Users/ariellestern/anaconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (4,7,8,11) have mixed types.Specify dtype option on import or set low_memory=False.\n","  interactivity=interactivity, compiler=compiler, result=result)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>InventoryID</th>\n","      <th>Glottocode</th>\n","      <th>ISO6393</th>\n","      <th>LanguageName</th>\n","      <th>SpecificDialect</th>\n","      <th>GlyphID</th>\n","      <th>Phoneme</th>\n","      <th>Allophones</th>\n","      <th>Marginal</th>\n","      <th>SegmentClass</th>\n","      <th>...</th>\n","      <th>retractedTongueRoot</th>\n","      <th>advancedTongueRoot</th>\n","      <th>periodicGlottalSource</th>\n","      <th>epilaryngealSource</th>\n","      <th>spreadGlottis</th>\n","      <th>constrictedGlottis</th>\n","      <th>fortis</th>\n","      <th>raisedLarynxEjective</th>\n","      <th>loweredLarynxImplosive</th>\n","      <th>click</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>kore1280</td>\n","      <td>kor</td>\n","      <td>Korean</td>\n","      <td>NaN</td>\n","      <td>68</td>\n","      <td>h</td>\n","      <td>ç h ɦ</td>\n","      <td>NaN</td>\n","      <td>consonant</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>+</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>kore1280</td>\n","      <td>kor</td>\n","      <td>Korean</td>\n","      <td>NaN</td>\n","      <td>006A</td>\n","      <td>j</td>\n","      <td>j</td>\n","      <td>NaN</td>\n","      <td>consonant</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>+</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>kore1280</td>\n","      <td>kor</td>\n","      <td>Korean</td>\n","      <td>NaN</td>\n","      <td>006B</td>\n","      <td>k</td>\n","      <td>k̚ ɡ k</td>\n","      <td>NaN</td>\n","      <td>consonant</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>kore1280</td>\n","      <td>kor</td>\n","      <td>Korean</td>\n","      <td>NaN</td>\n","      <td>006B+02B0</td>\n","      <td>kʰ</td>\n","      <td>kʰ</td>\n","      <td>NaN</td>\n","      <td>consonant</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>+</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>kore1280</td>\n","      <td>kor</td>\n","      <td>Korean</td>\n","      <td>NaN</td>\n","      <td>006B+02C0</td>\n","      <td>kˀ</td>\n","      <td>kˀ</td>\n","      <td>NaN</td>\n","      <td>consonant</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>+</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","      <td>-</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 48 columns</p>\n","</div>"],"text/plain":["   InventoryID Glottocode ISO6393 LanguageName SpecificDialect    GlyphID  \\\n","0            1   kore1280     kor       Korean             NaN         68   \n","1            1   kore1280     kor       Korean             NaN       006A   \n","2            1   kore1280     kor       Korean             NaN       006B   \n","3            1   kore1280     kor       Korean             NaN  006B+02B0   \n","4            1   kore1280     kor       Korean             NaN  006B+02C0   \n","\n","  Phoneme Allophones Marginal SegmentClass  ... retractedTongueRoot  \\\n","0       h      ç h ɦ      NaN    consonant  ...                   0   \n","1       j          j      NaN    consonant  ...                   0   \n","2       k     k̚ ɡ k      NaN    consonant  ...                   0   \n","3      kʰ         kʰ      NaN    consonant  ...                   0   \n","4      kˀ         kˀ      NaN    consonant  ...                   0   \n","\n","  advancedTongueRoot periodicGlottalSource epilaryngealSource spreadGlottis  \\\n","0                  0                     -                  -             +   \n","1                  0                     +                  -             -   \n","2                  0                     -                  -             -   \n","3                  0                     -                  -             +   \n","4                  0                     -                  -             -   \n","\n","  constrictedGlottis fortis raisedLarynxEjective loweredLarynxImplosive click  \n","0                  -      -                    -                      -     -  \n","1                  -      -                    -                      -     -  \n","2                  -      -                    -                      -     -  \n","3                  -      -                    -                      -     -  \n","4                  +      -                    -                      -     -  \n","\n","[5 rows x 48 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"lRTRGgECPrwy"},"source":["# Generate a mapping from nationality to language spoken\n","nationalities_to_language = {'Irish': 'English',\n","                             'India': 'Hindi', \n","                             'USA': 'English (American)',\n","                             'Australia': 'English (Australian)',\n","                             'Canada': 'English', \n","                             'UK': 'English (British)', \n","                             'Norway': 'Norwegian',\n","                             'Italy': 'Italian',\n","                             'Sudan': 'Arabic',\n","                             'Mexico': 'Spanish',\n","                             'China': 'Standard Chinese; Mandarin',\n","                             'Switzerland': 'Swiss German',\n","                             'Guyana': 'English',\n","                             'Philippines':'Filipino',\n","                             'New Zealand': 'English (New Zealand)',\n","                             'Germany': 'German', \n","                             'Portugal': 'Portuguese (European)',\n","                             'Netherlands': 'Dutch',\n","                             'Pakistan': 'Urdu',\n","                             'Croatia': 'Croatian',\n","                             'South Korea': 'Korean',\n","                             'Sweden': 'Swedish',\n","                             'Russia': 'Russian',\n","                             'Poland': 'Polish',\n","                             'Sri Lanka': 'Sinhalese', \n","                             'Singapore': 'Mandarin Chinese',\n","                             'Chile': 'Spanish',\n","                             'Spain': 'Spanish',\n","                             'Israel':'Modern Hebrew',\n","                             'Brazil': 'Portuguese (Brazilian)',\n","                             'Trinidad and Tobago': 'English', \n","                             'Denmark': 'Danish',\n","                             'Austria': 'German', \n","                             'South Africa': 'English', \n","                             'Iran': 'Farsi'} "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"zUCsUSiIPrwz"},"source":["# Filter dataframe to only nationalities that will be encountered\n","phoible_df = phoible_df[phoible_df['LanguageName'].isin(list(nationalities_to_language.values()))]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"iM_aNq3sPrwz"},"source":["# Find all languages spoken within VoxCeleb\n","all_languages = list(phoible_df['LanguageName'].unique())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"m6jUbwbJPrw0"},"source":["all_phonemes = list(phoible_df['Phoneme'].unique())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"Uum7Sjj6Prw0"},"source":["# Define a mapping from language to phoneme \n","# Key is language and value is a set of phonemes within that language\n","phonemes_per_lang = {}\n","for j in range(len(all_languages)):\n","    phonemes_per_lang[all_languages[j]] = {}\n","    phonemes_in_lang = phoible_df[phoible_df['LanguageName'] == all_languages[j]]['Phoneme'].unique()\n","    phonemes_per_lang[all_languages[j]] = set()\n","    for i in range(len(phonemes_in_lang)):\n","        phonemes_per_lang[all_languages[j]].add(phonemes_in_lang[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"T1LH1FWBPrw0"},"source":["# Create a set of all the phonemes in English languages\n","eng_langs = ['English', 'English (American)','English (Australian)', \\\n","             'English (British)', 'English (New Zealand)']\n","english_phonemes = set()\n","for lang in eng_langs:\n","    english_phonemes.update(phonemes_per_lang[lang])\n","\n","# Define a mapping from English phonemes to allophones that may be present in tother languages\n","english_phonemes_to_allophones = {}\n","for phoneme in english_phonemes:\n","    english_phonemes_to_allophones[phoneme] = set(phoneme)                               \n","    for allophones in phoible_df[(phoible_df['Phoneme'] == phoneme) & (phoible_df['LanguageName'].isin(eng_langs))].Allophones:\n","        if pd.isnull(allophones) == False and allophones.isalnum():\n","            for allophone in allophones:\n","                english_phonemes_to_allophones[phoneme].add(allophone)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"4X3RBk4nPrw1"},"source":["# Define path to where wav files are located\n","wav_path = '/Users/ariellestern/Desktop/cis519_project/aus_wav/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"WpqQ6BOWPrw2"},"source":["def pull_id_wavs(wav_path, id):\n","    '''\n","    Function: Find paths to wav files within specified directory\n","    Inputs: \n","        - wav_path: string of directory where wav files are located\n","    Outputs:\n","        - wav_ls: list paths to individual wav files\n","    '''\n","    wav_ls = list()\n","    for path, subdirs, files in os.walk(wav_path + id):\n","        for name in files:\n","            wav_path = str(pathlib.PurePath(path, name))\n","            wav_ls.append(wav_path)\n","    return wav_ls\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"c5iex_3-Prw2"},"source":["# Get all the ids of Australian individuals \n","# NOTE: Becuase of the time it takes to generate these files, we generated phoneme npz files in batches by nationality\n","aus_ids = list(list(audio_class_df[audio_class_df['Nationality'].isin(['Australia'])].VoxCeleb1_ID))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4vlMXVOXPrw2"},"source":["# Remove corrupt files, if any\n","aus_ids.remove('id10155') # corrupt file\n","aus_ids.remove('id10347') # corrupt file"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"DXZH-SGhPrw3"},"source":["# Define dictionary from id to list of all wav file paths associated with that id  \n","all_wav_dict = {key: pull_id_wavs(wav_path, key) for key in  aus_ids} "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"tmtdWViNPrw3"},"source":["def get_key_english_phonemes_to_allophones(val):\n","    '''\n","    Function: Find English allophones of non-English phonemes\n","    Inputs: \n","        - val: a phoneme\n","    Outputs:\n","        - key: the allophone that phoneme is known as in English, if applicable\n","    '''\n","    for key, value in english_phonemes_to_allophones.items():\n","        if val in value:\n","            return key"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"QJeGhH2fPrw3"},"source":["import speech_recognition as sr\n","import eng_to_ipa as p \n","\n","\n","def audio_to_phonemes(audio_dict):\n","    '''\n","    Function: Convert an audio file to phonmes\n","    Inputs: \n","        - audio_dict: a dictionary from id to a list of filepaths for wav files associated with that id \n","    Outputs:\n","        - phonemes_dict: a dictionary from id to a list of a sets of phonemes associated with recordings for that id\n","    '''\n","    num_ids = len(audio_dict)\n","    i = 0\n","    phonemes_dict = {}\n","    r = sr.Recognizer()\n","    for key in audio_dict:\n","        i += 1\n","        print(\"Working on id \"+str(i)+\" out of \"+ str(num_ids))\n","        print(key)\n","        phonemes_list = list()\n","        for wav_file in audio_dict[key]:\n","            audio_file = sr.AudioFile(wav_file)\n","            with audio_file as source: \n","                try:\n","                    audio = r.record(audio_file)\n","                    text = r.recognize_google(audio)\n","                except:\n","                    text = \"\"\n","            phonemes = p.convert(text)\n","            phonemes = phonemes.replace(\" \", \"\")\n","            phonemes_in_sample = set()\n","            for char in phonemes:\n","                key_phoneme = get_key_english_phonemes_to_allophones(char)\n","                if key_phoneme == 'unseen':\n","                    print('hit unseen')\n","                if key_phoneme != None:\n","                    phonemes_in_sample.add(key_phoneme)\n","            phonemes_list.append(phonemes_in_sample)\n","        phonemes_dict[key] = phonemes_list\n","        print('num files for id ')\n","        print(len(phonemes_dict[key]))\n","    return phonemes_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bXSDu2KdPrw4","outputId":"cda9665a-f635-45e3-ace0-4d1630d432b9"},"source":["phonemes_dict = audio_to_phonemes(all_wav_dict)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Working on id 1 out of 5\n","id11094\n","num files for id \n","51\n","Working on id 2 out of 5\n","id11173\n","num files for id \n","265\n","Working on id 3 out of 5\n","id11179\n","num files for id \n","65\n","Working on id 4 out of 5\n","id11192\n","num files for id \n","86\n","Working on id 5 out of 5\n","id11240\n","num files for id \n","72\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"yTmIFAvKPrw5"},"source":["# Save phonemes per id to .npz format\n","for id in list(phonemes_dict.keys()):\n","    filename = id + '.npz'\n","    data_to_store = phonemes_dict[id]\n","    np.savez(filename, *data_to_store)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"iPMHwQDAPrw5"},"source":[""],"execution_count":null,"outputs":[]}]}