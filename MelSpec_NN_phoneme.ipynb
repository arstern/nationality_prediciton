{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"colab":{"name":"MelSpec_NN_phoneme.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"collapsed":true,"id":"AUAJWH9y6OYm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608529167965,"user_tz":300,"elapsed":29660,"user":{"displayName":"Vera Lee","photoUrl":"","userId":"18328789372001276671"}},"outputId":"a3360e68-f562-41fa-b9e7-0d1d072255d0"},"source":["#Import required packages\n","import pandas as pd\n","import numpy as np\n","import librosa\n","import pathlib\n","import os\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import scale\n","import warnings\n","import glob\n","from scipy import signal\n","import tqdm\n","import tensorflow as tf\n","from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount = True)\n","%cd \"/content/drive/Shareddrives/CIS_519_Final_Project\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/Shareddrives/CIS_519_Final_Project\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"4AcX48YH6OYp","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1608483454915,"user_tz":300,"elapsed":687,"user":{"displayName":"Vera Lee","photoUrl":"","userId":"18328789372001276671"}},"outputId":"3c3773a1-b970-4136-96d3-783c66fdbdc1"},"source":["#Read audio classification file and clean\n","audio_class_df = pd.read_csv(\"audioclassification_meta.csv\")\n","c_names = audio_class_df.columns.tolist()\n","c_names = c_names[0].replace(\" \", \"_\").split(\"\\t\")\n","\n","audio_class_df[c_names] = audio_class_df['VoxCeleb1 ID\\tVGGFace1 ID\\tGender\\tNationality\\tSet'].\\\n","                        str.split(\"\\t\", expand = True)\n","audio_class_df = audio_class_df[c_names]\n","\n","#Set as dictionary\n","audio_class_dict = audio_class_df.set_index(\"VoxCeleb1_ID\").T.to_dict('list')\n","\n","#View data\n","audio_class_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>VoxCeleb1_ID</th>\n","      <th>VGGFace1_ID</th>\n","      <th>Gender</th>\n","      <th>Nationality</th>\n","      <th>Set</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>id10001</td>\n","      <td>A.J._Buckley</td>\n","      <td>m</td>\n","      <td>Ireland</td>\n","      <td>dev</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>id10002</td>\n","      <td>A.R._Rahman</td>\n","      <td>m</td>\n","      <td>India</td>\n","      <td>dev</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>id10003</td>\n","      <td>Aamir_Khan</td>\n","      <td>m</td>\n","      <td>India</td>\n","      <td>dev</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>id10004</td>\n","      <td>Aaron_Tveit</td>\n","      <td>m</td>\n","      <td>USA</td>\n","      <td>dev</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>id10005</td>\n","      <td>Aaron_Yoo</td>\n","      <td>m</td>\n","      <td>USA</td>\n","      <td>dev</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  VoxCeleb1_ID   VGGFace1_ID Gender Nationality  Set\n","0      id10001  A.J._Buckley      m     Ireland  dev\n","1      id10002   A.R._Rahman      m       India  dev\n","2      id10003    Aamir_Khan      m       India  dev\n","3      id10004   Aaron_Tveit      m         USA  dev\n","4      id10005     Aaron_Yoo      m         USA  dev"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"9VgLa-P86OYq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608483459124,"user_tz":300,"elapsed":1440,"user":{"displayName":"Vera Lee","photoUrl":"","userId":"18328789372001276671"}},"outputId":"992ee0a0-c29a-4be7-aa00-bf255f243c2a"},"source":["# Phoneme data needed to extract phonemes only\n","phoible_df = pd.read_csv(\"phoible.csv\")\n","\n","# Generate a mapping from nationality to language spoken\n","nationalities_to_language = {'Irish': 'English',\n","                             'India': 'Hindi', \n","                             'USA': 'English (American)',\n","                             'Australia': 'English (Australian)',\n","                             'Canada': 'English', \n","                             'UK': 'English (British)', \n","                             'Norway': 'Norwegian',\n","                             'Italy': 'Italian',\n","                             'Sudan': 'Arabic',\n","                             'Mexico': 'Spanish',\n","                             'China': 'Standard Chinese; Mandarin',\n","                             'Switzerland': 'Swiss German',\n","                             'Guyana': 'English',\n","                             'Philippines':'Filipino',\n","                             'New Zealand': 'English (New Zealand)',\n","                             'Germany': 'German', \n","                             'Portugal': 'Portuguese (European)',\n","                             'Netherlands': 'Dutch',\n","                             'Pakistan': 'Urdu',\n","                             'Croatia': 'Croatian',\n","                             'South Korea': 'Korean',\n","                             'Sweden': 'Swedish',\n","                             'Russia': 'Russian',\n","                             'Poland': 'Polish',\n","                             'Sri Lanka': 'Sinhalese', \n","                             'Singapore': 'Mandarin Chinese',\n","                             'Chile': 'Spanish',\n","                             'Spain': 'Spanish',\n","                             'Israel':'Modern Hebrew',\n","                             'Brazil': 'Portuguese (Brazilian)',\n","                             'Trinidad and Tobago': 'English', \n","                             'Denmark': 'Danish',\n","                             'Austria': 'German', \n","                             'South Africa': 'English', \n","                             'Iran': 'Farsi'} \n","\n","# Filter dataframe to only nationalities that will be encountered\n","phoible_df = phoible_df[phoible_df['LanguageName'].isin(list(nationalities_to_language.values()))]\n","\n","# Find all languages spoken within VoxCeleb\n","all_languages = list(phoible_df['LanguageName'].unique())\n","\n","# Define a mapping from language to phoneme \n","# Key is language and value is a set of phonemes within that language\n","phonemes_per_lang = {}\n","for j in range(len(all_languages)):\n","    phonemes_per_lang[all_languages[j]] = {}\n","    phonemes_in_lang = phoible_df[phoible_df['LanguageName'] == all_languages[j]]['Phoneme'].unique()\n","    phonemes_per_lang[all_languages[j]] = set()\n","    for i in range(len(phonemes_in_lang)):\n","        phonemes_per_lang[all_languages[j]].add(phonemes_in_lang[i])\n","\n","# Create a set of all the phonemes in English languages\n","eng_langs = ['English', 'English (American)','English (Australian)', \\\n","             'English (British)', 'English (New Zealand)']\n","english_phonemes = set()\n","for lang in eng_langs:\n","    english_phonemes.update(phonemes_per_lang[lang])\n","\n","# Define a mapping from English phonemes to allophones that may be present in tother languages\n","english_phonemes_to_allophones = {}\n","for phoneme in english_phonemes:\n","    english_phonemes_to_allophones[phoneme] = set(phoneme)                               \n","    for allophones in phoible_df[(phoible_df['Phoneme'] == phoneme) & (phoible_df['LanguageName'].isin(eng_langs))].Allophones:\n","        if pd.isnull(allophones) == False and allophones.isalnum():\n","            for allophone in allophones:\n","                english_phonemes_to_allophones[phoneme].add(allophone)\n","\n","def get_key_english_phonemes_to_allophones(val):\n","    '''\n","    Function: Find English allophones of non-English phonemes\n","    Inputs: \n","        - val: a phoneme\n","    Outputs:\n","        - key: the allophone that phoneme is known as in English, if applicable\n","    '''\n","    for key, value in english_phonemes_to_allophones.items():\n","        if val in value:\n","            return key\n","\n","# Define phonemes that will be unseen in training as they are non-English and \n","# remove them from a language's phoneme set, replaced by an unseen tag\n","for language in phonemes_per_lang:\n","    unseen_phonemes = set()\n","    for phoneme in phonemes_per_lang[language]:\n","        if phoneme not in english_phonemes:\n","            unseen_phonemes.add(phoneme) \n","    for unseen_phoneme in unseen_phonemes:\n","        phonemes_per_lang[language].remove(unseen_phoneme)\n","        possible_allophone = get_key_english_phonemes_to_allophones(unseen_phoneme)\n","        if possible_allophone is not None:\n","            phonemes_per_lang[language].add(possible_allophone)\n","        else:\n","            phonemes_per_lang[language].add('unseen')\n","\n","# Bypass languages to map directly from nationality to phoneme\n","nationalities_to_phonemes = {}\n","for nationality in nationalities_to_language.keys():\n","    nationalities_to_phonemes[nationality] = \\\n","    phonemes_per_lang[nationalities_to_language[nationality]]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (4,7,8,11) have mixed types.Specify dtype option on import or set low_memory=False.\n","  interactivity=interactivity, compiler=compiler, result=result)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"collapsed":true,"id":"CVUCDxRD6OYr"},"source":["# Path to audio npz files\n","speech_path = \"/content/drive/Shareddrives/CIS_519_Final_Project/\"\n","# Path to phoneme npz files\n","phoneme_path = \"/content/drive/Shareddrives/CIS_519_Final_Project/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"mFZBRQH96OYs"},"source":["def pull_id_npz(file_name):\n","    '''\n","    Function: Load in files in npz format that are stored as dictionaries\n","    Inputs: \n","        - file_name: string of file name containing path to file\n","    Outputs:\n","        - container_list: nested list containing values in the npz files\n","    '''\n","    container_list = []\n","    container = np.load(file_name,allow_pickle=True)\n","    container_list.append([container[key] for key in container])\n","    return container_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"3IlPsdF86OYs"},"source":["# Unpack npz files storing raw speech\n","def pull_speech_npz(speech_path, id):\n","    '''\n","    Function: Unpack npz files storing raw audio\n","    Inputs: \n","        - speech_path: path to audio npz files\n","    Outputs:\n","        - nested list containing values in the npz files \n","    '''\n","    for path, subdirs, files in os.walk(speech_path):\n","        if \"phoneme\" not in path:\n","            os.chdir(path)\n","        if (id+'.npz') in os.listdir():\n","            return pull_id_npz(id+'.npz')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"3lXcWJMQ6OYs"},"source":["# Pull all the phoneme npz files associated with a given Voxceleb id\n","def pull_phoneme_npz(phoneme_path, id):\n","    '''\n","    Function: Unpack npz files storing phonemes\n","    Inputs: \n","        - speech_path: path to phoneme npz files\n","    Outputs:\n","        - nested list containing values in the npz files \n","    '''\n","    for path, subdirs, files in os.walk(phoneme_path):\n","        if \"phoneme\" in path:\n","            os.chdir(path)\n","        if (id+'.npz') in os.listdir():\n","            return pull_id_npz(id+'.npz')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"HA6sNJla6OYt"},"source":["#from python_speech_features import fbank\n","# Function to calculate melspectrograms\n","def compute_melspec(data_to_ids_dict, sr = 16000):\n","    '''\n","    Function: Calculate melspectrogram from raw audio npz\n","    Inputs: \n","        - data_to_ids_dict: dictionary of ids to raw audio extracted from npz file\n","    Outputs:\n","        - X_melspec: list of averaged melspectrograms per audio file\n","    '''\n","    X_melspec = []\n","    for key in data_to_ids_dict:\n","        if data_to_ids_dict[key] is not None:\n","            for data in data_to_ids_dict[key]:\n","                #Get melspectrogram features, give it time series data, take mean across 0 axis to get 1x128 vector\n","                melspec = np.mean(librosa.feature.melspectrogram(y=data, sr=sr).T,axis=0)\n","                X_melspec.append(melspec)\n","    return X_melspec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"jklwTEj-6OYt"},"source":["def extract_phonemes_for_training(phoneme_dict):\n","    '''\n","    Function: Extract phonemes per training instance \n","    Inputs: \n","        - phoneme_dict: dictionary of ids to phoneme sets extracted from npz file\n","    Outputs:\n","        - X: list of sets of extracted phonemes, 1 set per file \n","    '''\n","    X = []\n","    for key in phoneme_dict.keys():\n","        for item in phoneme_dict[key][0]:\n","            X.append(item.item())\n","    return X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"L_c-5pB_6OYu"},"source":["def extract_nationalities(npz_to_speech_ids_dict, audio_class_dict):\n","    '''\n","    Function: Extract nationalities per training instance \n","    Inputs: \n","        - npz_to_speech_ids_dict: dictionary of ids to raw audio extracted from npz file\n","        - audio_class_dict: dictionary of id to nationality \n","    Outputs:\n","        - X: list of sets of extracted nationalities, 1 nationality per file \n","    '''\n","    y = []\n","    for key in npz_to_speech_ids_dict.keys():\n","        y_val = audio_class_dict[key][2]\n","        if npz_to_speech_ids_dict[key] is not None:\n","            for i in range(len(npz_to_speech_ids_dict[key][0])):\n","                y.append(y_val)\n","    return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"vi3M5ESm6OYu"},"source":["# Define all nationalities as English or non English speaking\n","all_nationalities = list(audio_class_df['Nationality'].unique())\n","# Remove English speaking nationalities with only 1 speaker in the set \n","all_nationalities.remove('South Africa')\n","all_nationalities.remove('Guyana')\n","all_nationalities.remove('Trinidad and Tobago')\n","all_nationalities.remove('Germany') # difficulty generating npz files here\n","\n","\n","eng_nationalities =  ['USA', 'UK', 'Australia', 'Canada', 'New Zealand', 'Ireland']\n","non_eng_nationalities = [nationality for nationality in all_nationalities if nationality not in eng_nationalities]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hpgrgsvEAtuz"},"source":["#Pre-Processing: Filtering\n","\n","#Sampling rate of audio data\n","fs = 16000\n","\n","def butter_lowpass(data, lowcut = 5000, fs=16000, order = 4):\n","    '''\n","    Function: Apply lowpass butterworth filter\n","    Inputs:\n","        - data: numpy array of wave data\n","        - lowcut: cutoff frequency, default set to 5000 Hz \n","        - fs: sampling rate, default set to 16000 Hz (based on VoxCeleb data)\n","        - order: filter order, default set to 4. Larger order = sharper cutoff\n","    Output:\n","        - y: numpy array, filtered version of wave data\n","    '''\n","    nyq = 0.5 * fs\n","    low = lowcut / nyq\n","    b, a = signal.butter(order, low, btype='low')\n","    y = signal.lfilter(b, a, data)\n","    return y\n","\n","def butter_highpass(data, highcut = 75, fs=16000, order = 4):\n","    '''\n","    Function: Apply lowpass butterworth filter\n","    Inputs:\n","        - data: numpy array of wave data\n","        - highcut: cutoff frequency, default set to 75 Hz (below this is mostly noise)\n","        - fs: sampling rate, default set to 16000 Hz (based on VoxCeleb data)\n","        - order: filter order, default set to 4. Larger order = sharper cutoff\n","    Output:\n","        - y: numpy array, filtered version of wave data\n","    '''\n","    nyq = 0.5 * fs\n","    high = highcut / nyq\n","    b, a = signal.butter(order, high, btype='high')\n","    y = signal.lfilter(b, a, data)\n","    return y\n","\n","def filter_signals(all_npz_dict):\n","    '''\n","    Function: Filter all signals in npz dictionaries with lowpass and highpass filters\n","    Inputs: \n","        - all_npz_dict: dictionary keyed by folders, contains lists of numpy arrays representing wave files\n","    Outputs:\n","        - filtered_npz: dictionary keyed by folders, contains list of numpy arrays representing filtered wav files\n","    '''\n","    filtered_npz = {}\n","    #Loop through all keys in dictionary\n","    for key in all_npz_dict:\n","        #Get list of all unfiltered data (all_npz_dict has two nested lists of data)\n","        unfiltered_list = all_npz_dict[key][0]\n","        filtered_list = []\n","        #Loop through all wave files\n","        for j, wave in enumerate(unfiltered_list):\n","            #Pass through lowpass and highpass filters\n","            filtered_wave = butter_lowpass(wave, 5000, 16000, 5)\n","            filtered_wave = butter_highpass(filtered_wave, 75, 16000, 5)\n","            filtered_list.append(filtered_wave)\n","        filtered_npz[key] = filtered_list\n","    return filtered_npz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"p3uNvBmp6OYu"},"source":["#from python_speech_features import mfcc\n","\n","def generate_zero_shot_data(nationalities):\n","    '''\n","    Function: Extract MFCCs and nationalities for ids belonging to a nationality within \"nationalities\"\n","    Inputs: \n","        - nationalities: a list of nationalities for which to determine MFCCs and nationalities\n","    Outputs:\n","        - X_melspec_train: list of averaged melspectrogram for each audio example\n","        - y_nationality_train_flat: list of the nationality corresponding to a given example \n","    '''\n","    X_melspec_train = []\n","    y_nationality_train = []\n","    for nationality in nationalities:\n","        print('Working on importing ' + nationality)\n","        speech_to_phoneme_training_ids = list(audio_class_df[audio_class_df['Nationality'].isin\\\n","                               ([nationality])].VoxCeleb1_ID)\n","        # Remove corrupt files and limit input sizes\n","        if 'id11240' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id11240')\n","        if 'id10155' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10155') \n","        if 'id10347' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10347')\n","        if 'id10409' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10409')\n","        if 'id10061' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10061') \n","        if len(speech_to_phoneme_training_ids) > 50:\n","            speech_to_phoneme_training_ids = speech_to_phoneme_training_ids[0:49]\n","            \n","        # Define dictionary from id to raw audio        \n","        npz_to_speech_ids_dict = {id: pull_speech_npz(speech_path,id) for id in speech_to_phoneme_training_ids}\n","        #Filter signals\n","        filtered_npz = filter_signals(npz_to_speech_ids_dict)\n","        # Compute melspectrograms off of audio\n","        melspec = np.array(compute_melspec(filtered_npz))\n","        X_melspec_train.append(melspec)\n","        # Extract nationalities from ids \n","        y_nationality = extract_nationalities(npz_to_speech_ids_dict, audio_class_dict)\n","        y_nationality_train.append(y_nationality)\n","        # Delete large files to clear memory\n","        del npz_to_speech_ids_dict\n","        del filtered_npz\n","        del melspec\n","        del y_nationality\n","    # Flatten lists\n","    X_melspec_train_flat = [item for sublist in X_melspec_train for item in sublist]\n","    y_nationality_train_flat = [item for sublist in y_nationality_train for item in sublist]\n","    return X_melspec_train_flat, y_nationality_train_flat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"yCDyDVrC6OYv"},"source":["def generate_phoneme_data(nationalities):\n","    '''\n","    Function: Extract MFCCs and nationalities for ids belonging to a nationality within \"nationalities\"\n","    Inputs: \n","        - nationalities: a list of nationalities for which to determine MFCCs and nationalities\n","    Outputs:\n","        - X_mfcc_train_flat: list of 10,000 MFCC coefficients per example \n","        - y_nationality_train_flat: list of the nationality corresponding to a given example \n","    '''\n","    y_phoneme_train = []\n","    for nationality in nationalities:\n","        print('Working on importing ' + nationality)\n","        speech_to_phoneme_training_ids = list(audio_class_df[audio_class_df['Nationality'].isin\\\n","                               ([nationality])].VoxCeleb1_ID)\n","        # Remove corrupt files and limit input sizes\n","        if 'id11240' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id11240')\n","        if 'id10155' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10155') \n","        if 'id10347' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10347')\n","        if 'id10409' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10409')\n","        if 'id10061' in speech_to_phoneme_training_ids:\n","            speech_to_phoneme_training_ids.remove('id10061') \n","        if len(speech_to_phoneme_training_ids) > 50:\n","            speech_to_phoneme_training_ids = speech_to_phoneme_training_ids[0:49]\n","        # Define dictionary from id to raw audio \n","        npz_to_speech_ids_dict = {id: pull_speech_npz(speech_path,id) for id in speech_to_phoneme_training_ids}\n","        # Extract nationalities from ids \n","        phoneme_ids_dict ={id: pull_phoneme_npz(phoneme_path,id) for id in speech_to_phoneme_training_ids}\n","        y_phonemes = extract_phonemes_for_training(phoneme_ids_dict)    \n","        y_phoneme_train.append(y_phonemes)\n","        # Delete large files to clear memory\n","        del speech_to_phoneme_training_ids\n","        del y_phonemes\n","    # Flatten list\n","    y_phoneme_train_flat = [item for sublist in y_phoneme_train for item in sublist]\n","    return y_phoneme_train_flat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"gMv-SICB6OYv"},"source":["# Generate training and testing data\n","X_train, y_train_nationalities = generate_zero_shot_data(eng_nationalities)\n","X_test, y_test_nationalities = generate_zero_shot_data(non_eng_nationalities)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BNp2MDjwPmZS","executionInfo":{"status":"ok","timestamp":1608486298344,"user_tz":300,"elapsed":21365,"user":{"displayName":"Vera Lee","photoUrl":"","userId":"18328789372001276671"}},"outputId":"6710a189-7c88-44b1-e1c0-68208388bdae"},"source":["#Load in phoneme data\n","%cd \"/content/drive/Shareddrives/CIS_519_Final_Project/train_test_data_to_load_in\"\n","y_train_phonemes =  pull_id_npz('y_train_phonemes_no_corrupt.npz')\n","y_train_phonemes = y_train_phonemes[0]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/Shareddrives/CIS_519_Final_Project/train_test_data_to_load_in\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Qo2WjyjOByeV"},"source":["def one_hot_encode_phonemes(y_phoneme_sets, all_phonemes_list):\n","    '''\n","    Function: One hot encode a set of phonemes\n","    Inputs: \n","        - y_phoneme_sets: a list of sets, each of which contains the phonemes present in a given instance\n","        - all_phonemes_list: a list of all the possible phonemes present, generally a list of English phonemes\n","    Outputs:\n","        - y_phonemes_encoded: nested list containing encodings of phoneme sets\n","    '''    \n","    num_phonemes = len(all_phonemes_list)\n","    y_phonemes_encoded = []\n","    for example in y_phoneme_sets:\n","        y_phonemes_example_encoded = [0]*num_phonemes\n","        for i in range(len(all_phonemes_list)):\n","            if all_phonemes_list[i] in example.item():\n","                y_phonemes_example_encoded[i] = 1\n","        y_phonemes_encoded.append(y_phonemes_example_encoded)\n","    return y_phonemes_encoded\n","\n","\n","def un_one_hot_encode_phonemes(y_phoneme_encoded, all_phonemes_list):\n","    '''\n","    Function: Reverse one hot encode a set of phonemes\n","    Inputs: \n","        - y_phoneme_encoded: a nested list, contains the phonemes present in a given instance as a one hot encoding\n","        - all_phonemes_list: a list of all the possible phonemes present, generally a list of English phonemes\n","    Outputs:\n","        - y_phonemes_unencoded: a list of sets, each of which contains the phonemes present in a given instance\n","    '''  \n","    num_phonemes = len(all_phonemes_list)\n","    y_phonemes_unencoded = []\n","    for example in y_phoneme_encoded:\n","        y_phonemes_example_unencoded = set()\n","        for i in range(num_phonemes):\n","            if example[i] == 1:\n","                y_phonemes_example_unencoded.add(all_phonemes_list[i])\n","        y_phonemes_unencoded.append(y_phonemes_example_unencoded)\n","    return y_phonemes_unencoded\n","\n","# Add the unseen tag to the set of English phonemes and define the number of classes for the LSTM model\n","english_phonemes.add('unseen')\n","n_classes = len(english_phonemes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"02nl-C_hB__j"},"source":["# Convert to array\n","X_train_arr = np.asarray(X_train)\n","y_train_phonemes_arr = np.asarray(y_train_phonemes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2WL-GVr4CDx6"},"source":["from sklearn.model_selection import train_test_split\n","# Split data into training and validation set\n","X_train_arr, X_valid_arr, y_train_phonemes_arr, y_valid_phonemes_arr = \\\n","train_test_split(X_train_arr, y_train_phonemes_arr, test_size=0.4, random_state=42) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5OumIc8CCIsX"},"source":["#One hot encode phonemes\n","all_phonemes = list(english_phonemes)\n","y_train_phonemes_arr = one_hot_encode_phonemes(y_train_phonemes_arr, all_phonemes)\n","y_train_phonemes_arr = np.asarray(y_train_phonemes_arr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"339NRVm3Mogg"},"source":["#NN Model for Predicting Phonemes\n","\n","def create_model():\n","    '''\n","    Function: 12-layer dense feed-forward neural network for predicting phonemes\n","    Input: None\n","    Outputs:\n","        - NNmodel: neural network framework\n","    '''\n","\n","    #Input is size 128, dropout of 10% between each layer\n","    NNmodel = tf.keras.models.Sequential([                                     \n","    tf.keras.layers.Dense(256, activation='relu'),\n","    tf.keras.layers.Dropout(0.1),\n","    tf.keras.layers.Dense(256, activation='relu'),\n","    tf.keras.layers.Dropout(0.1),\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    tf.keras.layers.Dropout(0.1),\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    tf.keras.layers.Dropout(0.1),\n","    tf.keras.layers.Dense(1028, activation='relu'),\n","    tf.keras.layers.Dropout(0.1),\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    tf.keras.layers.Dropout(0.1),\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    tf.keras.layers.Dropout(0.1),\n","    tf.keras.layers.Dense(256, activation='relu'),\n","    tf.keras.layers.Dropout(0.1),\n","    tf.keras.layers.Dense(256, activation='relu'),\n","    tf.keras.layers.Dropout(0.1),\n","    tf.keras.layers.Dense(128, activation='relu'),\n","    tf.keras.layers.Dropout(0.1),\n","    tf.keras.layers.Dense(128, activation='relu'),\n","    tf.keras.layers.Dropout(0.1),\n","    tf.keras.layers.Dense(n_classes, activation='sigmoid')])\n","\n","    NNmodel.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n","    return NNmodel"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rKbk2mytNFbF","executionInfo":{"status":"ok","timestamp":1608510489038,"user_tz":300,"elapsed":205721,"user":{"displayName":"Vera Lee","photoUrl":"","userId":"18328789372001276671"}},"outputId":"abf2e309-6be9-4402-be33-cc57a2ec62a6"},"source":["phoneme_NNmodel = create_model()\n","\n","#Tensorboard to view losses and accuracies\n","tensorboard = TensorBoard(log_dir=\"logs\")\n","# Stop training if in 5 epochs accuracy is not improving, save weights that get best accuracy\n","early_stopping = EarlyStopping(mode=\"min\", patience=5, restore_best_weights=True)\n","\n","#Run on maximum of 100 epochs (usually only takes about 45 to converge)\n","history = phoneme_NNmodel.fit(X_train_arr, y_train_phonemes_arr, epochs=100, batch_size=64, validation_split=0.1, callbacks=[tensorboard, early_stopping])\n","\n","#Get summary of model \n","phoneme_NNmodel.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","209/209 [==============================] - 9s 35ms/step - loss: 0.4345 - accuracy: 0.0025 - val_loss: 0.3281 - val_accuracy: 0.0000e+00\n","Epoch 2/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3316 - accuracy: 1.5567e-04 - val_loss: 0.3281 - val_accuracy: 0.0000e+00\n","Epoch 3/100\n","209/209 [==============================] - 7s 34ms/step - loss: 0.3292 - accuracy: 2.8334e-04 - val_loss: 0.3259 - val_accuracy: 0.0000e+00\n","Epoch 4/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3268 - accuracy: 1.9226e-04 - val_loss: 0.3251 - val_accuracy: 0.0000e+00\n","Epoch 5/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3251 - accuracy: 3.7808e-05 - val_loss: 0.3242 - val_accuracy: 0.0000e+00\n","Epoch 6/100\n","209/209 [==============================] - 7s 34ms/step - loss: 0.3243 - accuracy: 3.4404e-05 - val_loss: 0.3249 - val_accuracy: 0.0000e+00\n","Epoch 7/100\n","209/209 [==============================] - 7s 35ms/step - loss: 0.3231 - accuracy: 2.2719e-04 - val_loss: 0.3252 - val_accuracy: 0.0000e+00\n","Epoch 8/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3233 - accuracy: 6.4566e-05 - val_loss: 0.3226 - val_accuracy: 0.0000e+00\n","Epoch 9/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3211 - accuracy: 1.8161e-04 - val_loss: 0.3223 - val_accuracy: 0.0000e+00\n","Epoch 10/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3212 - accuracy: 1.6747e-04 - val_loss: 0.3227 - val_accuracy: 0.0000e+00\n","Epoch 11/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3205 - accuracy: 1.1273e-04 - val_loss: 0.3226 - val_accuracy: 0.0000e+00\n","Epoch 12/100\n","209/209 [==============================] - 7s 34ms/step - loss: 0.3198 - accuracy: 2.3964e-04 - val_loss: 0.3208 - val_accuracy: 0.0000e+00\n","Epoch 13/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3173 - accuracy: 5.9426e-04 - val_loss: 0.3217 - val_accuracy: 0.0000e+00\n","Epoch 14/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3170 - accuracy: 2.0147e-04 - val_loss: 0.3197 - val_accuracy: 0.0000e+00\n","Epoch 15/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3148 - accuracy: 5.2426e-04 - val_loss: 0.3183 - val_accuracy: 0.0000e+00\n","Epoch 16/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3141 - accuracy: 2.6438e-04 - val_loss: 0.3181 - val_accuracy: 0.0000e+00\n","Epoch 17/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3139 - accuracy: 0.0011 - val_loss: 0.3178 - val_accuracy: 0.0000e+00\n","Epoch 18/100\n","209/209 [==============================] - 7s 34ms/step - loss: 0.3135 - accuracy: 0.0017 - val_loss: 0.3175 - val_accuracy: 0.0000e+00\n","Epoch 19/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3122 - accuracy: 0.0015 - val_loss: 0.3180 - val_accuracy: 0.0000e+00\n","Epoch 20/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3109 - accuracy: 0.0015 - val_loss: 0.3182 - val_accuracy: 0.0128\n","Epoch 21/100\n","209/209 [==============================] - 7s 35ms/step - loss: 0.3099 - accuracy: 0.0074 - val_loss: 0.3172 - val_accuracy: 0.0000e+00\n","Epoch 22/100\n","209/209 [==============================] - 7s 34ms/step - loss: 0.3093 - accuracy: 8.6883e-04 - val_loss: 0.3157 - val_accuracy: 0.0000e+00\n","Epoch 23/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3081 - accuracy: 0.0016 - val_loss: 0.3161 - val_accuracy: 0.0000e+00\n","Epoch 24/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3056 - accuracy: 0.0040 - val_loss: 0.3151 - val_accuracy: 0.0000e+00\n","Epoch 25/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3059 - accuracy: 0.0042 - val_loss: 0.3159 - val_accuracy: 0.0067\n","Epoch 26/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3050 - accuracy: 0.0139 - val_loss: 0.3156 - val_accuracy: 0.0000e+00\n","Epoch 27/100\n","209/209 [==============================] - 7s 34ms/step - loss: 0.3042 - accuracy: 0.0027 - val_loss: 0.3154 - val_accuracy: 0.0000e+00\n","Epoch 28/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.3033 - accuracy: 0.0097 - val_loss: 0.3158 - val_accuracy: 0.0331\n","Epoch 29/100\n","209/209 [==============================] - 7s 33ms/step - loss: 0.2994 - accuracy: 0.0157 - val_loss: 0.3163 - val_accuracy: 0.0000e+00\n","Model: \"sequential_12\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_111 (Dense)            (None, 256)               33024     \n","_________________________________________________________________\n","dropout_99 (Dropout)         (None, 256)               0         \n","_________________________________________________________________\n","dense_112 (Dense)            (None, 256)               65792     \n","_________________________________________________________________\n","dropout_100 (Dropout)        (None, 256)               0         \n","_________________________________________________________________\n","dense_113 (Dense)            (None, 512)               131584    \n","_________________________________________________________________\n","dropout_101 (Dropout)        (None, 512)               0         \n","_________________________________________________________________\n","dense_114 (Dense)            (None, 512)               262656    \n","_________________________________________________________________\n","dropout_102 (Dropout)        (None, 512)               0         \n","_________________________________________________________________\n","dense_115 (Dense)            (None, 1028)              527364    \n","_________________________________________________________________\n","dropout_103 (Dropout)        (None, 1028)              0         \n","_________________________________________________________________\n","dense_116 (Dense)            (None, 512)               526848    \n","_________________________________________________________________\n","dropout_104 (Dropout)        (None, 512)               0         \n","_________________________________________________________________\n","dense_117 (Dense)            (None, 512)               262656    \n","_________________________________________________________________\n","dropout_105 (Dropout)        (None, 512)               0         \n","_________________________________________________________________\n","dense_118 (Dense)            (None, 256)               131328    \n","_________________________________________________________________\n","dropout_106 (Dropout)        (None, 256)               0         \n","_________________________________________________________________\n","dense_119 (Dense)            (None, 256)               65792     \n","_________________________________________________________________\n","dropout_107 (Dropout)        (None, 256)               0         \n","_________________________________________________________________\n","dense_120 (Dense)            (None, 128)               32896     \n","_________________________________________________________________\n","dropout_108 (Dropout)        (None, 128)               0         \n","_________________________________________________________________\n","dense_121 (Dense)            (None, 128)               16512     \n","_________________________________________________________________\n","dropout_109 (Dropout)        (None, 128)               0         \n","_________________________________________________________________\n","dense_122 (Dense)            (None, 92)                11868     \n","=================================================================\n","Total params: 2,068,320\n","Trainable params: 2,068,320\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"id":"GM6eZf3u7ClB","executionInfo":{"status":"ok","timestamp":1608510500828,"user_tz":300,"elapsed":540,"user":{"displayName":"Vera Lee","photoUrl":"","userId":"18328789372001276671"}},"outputId":"708ce848-dc0b-4a7b-f63e-5463a08f50be"},"source":["#Plot training and validation losses\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Phoneme Neural Network Model Loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper right')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"A7f1VWsXZ3Nt"},"source":["#Predict on the validation set\n","valid_predictions = phoneme_NNmodel.predict(X_valid_arr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FIPsM67WkPHo"},"source":["#Define Averaged Perceptron model\n","\n","class Classifier(object):\n","    \"\"\"\n","    The Classifier class is the base class for all of the Perceptron-based\n","    algorithms. Your class should override the \"process_example\" and\n","    \"predict_single\" functions. Further, the averaged models should\n","    override the \"finalize\" method, where the final parameter values\n","    should be calculated. \n","    \"\"\"\n","    \n","    #Iterations is member variable of class\n","    ITERATIONS = 10\n","    \n","    def train(self, X, y):\n","        for iteration in range(self.ITERATIONS):\n","            for x_i, y_i in zip(X, y):\n","                self.process_example(x_i, y_i)\n","        self.finalize()\n","\n","    def process_example(self, x, y):\n","        \"\"\"\n","        Makes a prediction using the current parameter values for\n","        the features x and potentially updates the parameters based\n","        on the gradient. Note \"x\" is a dictionary which maps from the feature\n","        name to the feature value and y is either 1 or -1.\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    def finalize(self):\n","        \"\"\"Calculates the final parameter values for the averaged models.\"\"\"\n","        pass\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predicts labels for all of the input examples. You should not need\n","        to override this method.\n","        \"\"\"\n","        return [self.predict_single(x) for x in X]\n","\n","    def predict_single(self, x):\n","        \"\"\"\n","        Predicts a label, 1 or -1, for the input example. \"x\" is a dictionary\n","        which maps from the feature name to the feature value.\n","        \"\"\"\n","        raise NotImplementedError\n","\n","class AveragedPerceptron(Classifier):\n","    def __init__(self, features):\n","        self.eta = 1\n","        \n","        #Current weight parameters and theta\n","        self.w = 0.0\n","        self.theta = 0\n","        \n","        #Vectors to hold all previous parameters and thetas summed, initialize as 0s\n","        self.sumw = 0.0\n","        self.sumtheta = 0\n","        \n","        #Initialize counter for number of examples looped through\n","        self.counter = 1\n","        \n","    def process_example(self, x, y):\n","        #Get either y_pred = -1 (if score leq 0) or = 1 (if score > 0)\n","        y_pred = self.predict_single(x)\n","        #If make a mistake (y != y_pred)\n","        if y != y_pred:\n","            #Loop through all features in example x\n","            #Update w normally\n","            self.w += self.eta * y * x\n","            #Update the summed parameters, multiplying by number of examples seen\n","            self.sumw += self.counter*self.eta*y*x\n","            #Update theta normally\n","            self.theta += self.eta * y\n","            #Update summed theta, multiplying by number of examples seen\n","            self.sumtheta += self.counter*self.eta*y\n","        #Increase counter (doesn't matter if error was made)\n","        self.counter += 1\n","\n","    #Predict either -1 or 1 label on example x\n","    def predict_single(self, x):\n","        score = 0\n","        #Loop through all features in example x\n","        #Calculate dot product - multiply weight for that feature times value of feature in x\n","        score += self.w * x\n","        #Add theta to score\n","        score += self.theta\n","        #If the score is less than or equal to 0, return y_pred = -1\n","        if score <= 0:\n","            return -1\n","        #else return positive 1\n","        return 1\n","      \n","    #Returns final parameter values for averaged model\n","    def finalize(self):\n","        #Loop through all parameters in w\n","        self.w -= (1/self.counter)*self.sumw\n","        #Update final theta by subtracting sumtheta/counter\n","        self.theta -= (1/self.counter)*self.sumtheta"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_GmniY7JSaxJ"},"source":["#Make predictions on test data\n","X_test_arr = np.asarray(X_test)\n","test_predictions = phoneme_NNmodel.predict(X_test_arr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x-LCi031pavf"},"source":["#One-hot encode validation labels\n","y_valid_phonemes_arr = one_hot_encode_phonemes(y_valid_phonemes_arr, all_phonemes)\n","y_valid_phonemes_arr = np.asarray(y_valid_phonemes_arr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0slzExAXkugI"},"source":["features = [str(i) for i in range(1)]\n","test_predictions_thresholded = test_predictions.copy()\n","#Run each phoneme through averaged Perceptron using validation as training data since phoneme content is known\n","for counter, j in enumerate(valid_predictions[0]):\n","    #Get data for just one phoneme\n","    X = valid_predictions[:, counter]\n","    y = y_valid_phonemes_arr[:, counter]\n","    #Change all 0s to -1s for averaged Perceptron model\n","    y[y==0] = -1\n","    test = test_predictions[:, counter]\n","    avgperceptronClass = AveragedPerceptron(features)\n","    #Train averaged Perceptron on validation data\n","    avgperceptronClass.train(X, y)\n","    #Predict on test data (outputted from FF-neural network)\n","    avgperceptron_pred = avgperceptronClass.predict(test)\n","    test_predictions_thresholded[:, counter] = avgperceptron_pred\n","    #Change all -1 back to 0 for one-hot-encoding\n","    test_predictions_thresholded[:, counter][test_predictions_thresholded[:, counter] == -1] = 0\n","\n","#Decode back to phonemes\n","test_predictions_unencoded = un_one_hot_encode_phonemes(test_predictions_thresholded, all_phonemes)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HyY9RzBfWyJG"},"source":["#Remove any nationalities not present in test data \n","test_nationalities_phonemes = {}\n","\n","for key in nationalities_to_phonemes:\n","    if key in all_nationalities:\n","        test_nationalities_phonemes[key] = nationalities_to_phonemes[key]\n","\n","del test_nationalities_phonemes['USA']\n","del test_nationalities_phonemes['Australia']\n","del test_nationalities_phonemes['Canada']\n","del test_nationalities_phonemes['UK']\n","del test_nationalities_phonemes['New Zealand']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wYi2XAACS4ER"},"source":["def make_predictions_overlap(nationalities_to_phonemes, y_phonemes):\n","    '''\n","    Function: Determine most likely nationality by maximizing Jaccard Index between predicted phonemes and \n","    phonemes associated with each nationality\n","    Inputs: \n","        - nationalities_to_phonemes: a mapping from nationalities to a set of associated phonemes\n","        - y_phonemes: list of predicted phonemes, each instance in list is a set of phonemes\n","    Outputs:\n","        - preds: list of most likely nationality for each instance \n","        - all_preds - list of dictionaries containing Jaccard Index scores for each nationality for each index \n","    '''\n","    preds = []\n","    all_preds = []\n","    for instance in y_phonemes:\n","        nationality_overlap = {}\n","        num_phonemes_in_instance = len(instance)\n","        for nationality in nationalities_to_phonemes:\n","            phoneme_overlap_count = 0\n","            num_phonemes_for_nationality = len(nationalities_to_phonemes[nationality])\n","            overlap = len(instance.intersection(nationalities_to_phonemes[nationality]))\n","            jaccard_index = overlap/(num_phonemes_in_instance+num_phonemes_for_nationality-overlap)\n","            nationality_overlap[nationality] = jaccard_index\n","        top_guess = max(nationality_overlap, key=nationality_overlap.get)\n","        preds.append(top_guess) \n","        all_preds.append(nationality_overlap)\n","    return preds, all_preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SsKpZ7Z2TEtD"},"source":["#Predict nationalities based on phoneme content\n","y_test_nationalities_pred, confs = make_predictions_overlap(test_nationalities_phonemes, test_predictions_unencoded)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lgyKNdwMTHkp"},"source":["def determine_accuracy_by_nationality(y_test_nationalities_list, y_test_nationalities_pred):\n","    '''\n","    Function: Determine accuracy in predicting nationality for each nationality\n","    Inputs: \n","        - y_test_nationalities_list: list of true nationalities\n","        - y_test_nationalities_pred: list of predicted nationalities\n","    Outputs:\n","        - acc_per_nationality_dict: dictionary of accuracy per nationality, keys are nationalities\n","    '''    \n","    correct_count_per_nationality_dict = {}\n","    tot_count_per_nationality_dict = {}\n","\n","    for nationality in y_test_nationalities_list:\n","        correct_count_per_nationality_dict[nationality] = 0\n","        tot_count_per_nationality_dict[nationality] = 0\n","\n","    for i in range(len(y_test_nationalities_list)):\n","        tot_count_per_nationality_dict[y_test_nationalities_list[i]] += 1\n","        if y_test_nationalities_list[i] == y_test_nationalities_pred[i]:\n","            correct_count_per_nationality_dict[y_test_nationalities_list[i]] += 1 \n","    acc_per_nationality_dict = {}\n","    for nationality in tot_count_per_nationality_dict.keys():\n","        acc_per_nationality_dict[nationality] = \\\n","        correct_count_per_nationality_dict[nationality]/tot_count_per_nationality_dict[nationality]\n","\n","    return acc_per_nationality_dict\n","\n","# Find accuracy per nationality on test set \n","determine_accuracy_by_nationality(y_test_nationalities, y_test_nationalities_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HKVAqeYysb0f"},"source":["#Get overall accuracy\n","from sklearn.metrics import accuracy_score\n","accuracy_score(y_test_nationalities, y_test_nationalities_pred)"],"execution_count":null,"outputs":[]}]}